{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from scipy.spatial.distance import cosine\n",
    "from pandas import json_normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import hstack\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, array, struct\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "MONGO_CONNECTION_STRING = os.getenv(\"MONGO_CONNECTION_STRING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pull data from mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_mongodb():\n",
    "    ''' set up mongodb atlas connection'''\n",
    "    \n",
    "    uri = \"mongodb+srv://rsnagarkar:Pallavi10@cluster1.wgyjd.mongodb.net\"\n",
    "    client = MongoClient(uri)\n",
    "\n",
    "    # send a ping to confirm a successful connection\n",
    "    try:\n",
    "        client.admin.command('ping')\n",
    "        print(\"Pinged your deployment. Connection successful!\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    db = client.medium_database\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. Connection successful!\n"
     ]
    }
   ],
   "source": [
    "db = connect_to_mongodb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(db):\n",
    "    '''fetch data from server'''\n",
    "    writer_data = pd.DataFrame(list(db.writer_data.find({})))\n",
    "    follower_data = pd.DataFrame(list(db.followers_data.find({})))\n",
    "    article_data = pd.DataFrame(list(db.articles_data.find({})))\n",
    "    print(f'fetched {len(writer_data)}+{len(follower_data)}+{len(article_data)} records')\n",
    "    return writer_data, follower_data, article_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched 22+68398+2618 records\n"
     ]
    }
   ],
   "source": [
    "writers_raw, followers, articles = get_data(db)\n",
    "# Remove the _id field as it's not needed for analysis\n",
    "# del followers['_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How I Won Singapore‚Äôs GPT-4 Prompt Engineering Competition\\n\\nA deep dive into the strategies I learned for harnessing the power of Large Language Models (LLMs)\\n\\nCelebrating a milestone - The real win was the priceless learning experience!\\n\\nLast month, I had the incredible honor of winning Singapore‚Äôs first ever GPT-4 Prompt Engineering competition, which brought together over 400 prompt-ly brilliant participants, organised by the Government Technology Agency of Singapore (GovTech).\\n\\nPrompt engineering is a discipline that blends both art and science - it is as much technical understanding as it is of creativity and strategic thinking. This is a compilation of the prompt engineering strategies I learned along the way, that push any LLM to do exactly what you need and more!\\n\\nAuthor‚Äôs Note:\\nIn writing this, I sought to steer away from the traditional prompt engineering techniques that have already been extensively discussed and documented online. Instead, my aim is to bring fresh insights that I learned through experimentation, and a different, personal take in understanding and approaching certain techniques. I hope you‚Äôll enjoy reading this piece!\\n\\nThis article covers the following, with üîµ referring to beginner-friendly prompting techniques while üî¥ refers to advanced strategies:\\n\\n1. [üîµ] Structuring prompts using the CO-STAR framework\\n\\n2. [üîµ] Sectioning prompts using delimiters\\n\\n3. [üî¥] Creating system prompts with LLM guardrails\\n\\n4. [üî¥] Analyzing datasets using only LLMs, without plugins or code - \\nWith a hands-on example of analyzing a real-world Kaggle dataset using GPT-4\\n\\n1. [üîµ] Structuring Prompts using the CO-STAR framework\\n\\nEffective prompt structuring is crucial for eliciting optimal responses from an LLM. The CO-STAR framework, a brainchild of GovTech Singapore‚Äôs Data Science & AI team, is a handy template for structuring prompts. It considers all the key aspects that influence the effectiveness and relevance of an LLM‚Äôs response, leading to more optimal responses.\\n\\nCO-STAR framework - Image by author\\n\\nHere‚Äôs how it works:\\n\\n(C) Context: Provide background information on the task\\n\\nThis helps the LLM understand the specific scenario being discussed, ensuring its response is relevant.\\n\\n(O) Objective: Define what the task is that you want the LLM to perform\\n\\nBeing clear about your objective helps the LLM to focus its response on meeting that specific goal.\\n\\n(S) Style: Specify the writing style you want the LLM to use\\n\\nThis could be a particular famous person‚Äôs style of writing, or a particular expert in a profession, like a business analyst expert or CEO. This guides the LLM to respond with the manner and choice of words aligned with your needs.\\n\\n(T) Tone: Set the attitude of the response\\n\\nThis ensures the LLM‚Äôs response resonates with the intended sentiment or emotional context required. Examples are formal, humorous, empathetic, among others.\\n\\n(A) Audience: Identify who the response is intended for\\n\\nTailoring the LLM‚Äôs response to an audience, such as experts in a field, beginners, children, and so on, ensures that it is appropriate and understandable in your required context.\\n\\n(R) Response: Provide the response format\\n\\nThis ensures that the LLM outputs in the exact format that you require for downstream tasks. Examples include a list, a JSON, a professional report, and so on. For most LLM applications which work on the LLM responses programmatically for downstream manipulations, a JSON output format would be ideal.\\n\\nA practical application of CO-STAR\\n\\nHere‚Äôs a real-world example of why the CO-STAR framework is useful. Let‚Äôs say you work as a social media manager, and you need help drafting a Facebook post to advertise your company‚Äôs new product.\\n\\nA quick prompt without CO-STAR might look something like this:\\n\\nWrite a facebook post to advertise my company‚Äôs new product. My company‚Äôs name is Alpha and the product is called Beta, a new ultra-fast hairdryer.\\n\\nHere‚Äôs the output from GPT-4:\\n\\nIntroducing the Revolutionary Beta Hairdryer by Alpha!\\n- Say goodbye to long, tedious hair drying sessions! Alpha is proud to unveil the Beta, our latest innovation in haircare technology.\\n- Ultra-Fast Drying: The Beta is designed to cut your drying time in half, thanks to its advanced, powerful motor. Get ready faster and enjoy more time doing what you love!\\nShop Now: [Link to product page]\\n\\nWhile adequate, this output is painfully generic, lacking the specific nuances and targeted appeal necessary to resonate with your company‚Äôs audience.\\n\\nHere‚Äôs an example of using the CO-STAR template instead, which reminds you to incorporate other aspects of your task into your prompt, specifically the STYLE, TONE, and AUDIENCE aspects that are missing from the previous quick prompt:\\n\\n# CONTEXT # \\nI want to advertise my company‚Äôs new product. My company‚Äôs name is Alpha and the product is called Beta, which is a new ultra-fast hairdryer.\\n\\n# OBJECTIVE #\\nCreate a Facebook post for me, which aims to get people to click on the product link to purchase it.\\n\\n# STYLE #\\nFollow the writing style of successful companies that advertise similar products, such as Dyson.\\n\\n# TONE #\\nPersuasive\\n\\n# AUDIENCE #\\nMy company‚Äôs audience profile on Facebook is typically the older generation. Tailor your post to target what this audience typically looks out for in hair products.\\n\\n# RESPONSE #\\nThe Facebook post, kept concise yet impactful.\\n\\nBy applying CO-STAR, GPT-4‚Äôs response becomes significantly more targeted and effective:\\n\\nEmbrace Effortless Elegance with Alpha Beta - The Ultra-Fast Hairdryer for the Wise Generation.\\n\\nRediscover the joy of simple, effective hair care with Alpha Beta. Our latest innovation is more than just a hairdryer; it‚Äôs a promise of swift, gentle, and \\nreliable hair styling for those who appreciate the finer things in life.\\n\\n- Easy and Intuitive Use: Say goodbye to complicated gadgets. Alpha Beta is \\ncrafted for comfort and simplicity, perfect for those who value straightforward,\\nhassle-free technology. \\n- Time-Saving Technology: We understand your time is precious. That‚Äôs why Alpha Beta cuts down drying time significantly, giving you more moments to enjoy life‚Äôs pleasures.\\n\\nMake every day a good hair day with Alpha Beta. Experience the blend of \\nsophistication and simplicity today.\\n\\n[Your Product Link Here]\\n\\nThe CO-STAR framework guides you to provide all of the crucial pieces of information about your task to the LLM in a structured manner, ensuring a tailored and optimized response to exactly what you need.\\n\\n2. [üîµ] Sectioning Prompts Using Delimiters\\n\\nImage generated by DALL¬∑E 3\\n\\nDelimiters are special tokens that help the LLM distinguish which parts of your prompt it should consider as a single unit of meaning. This is important because your entire prompt arrives to the LLM as a single long sequence of tokens. Delimiters provide structure to this sequence of tokens by fencing specific parts of your prompt to be treated differently.\\n\\nIt is noteworthy that delimiters may not make a difference to the quality of an LLM‚Äôs response for straightforward tasks. However, the more complex the task, the more impact the usage of delimiters for sectioning has on the LLM‚Äôs response.\\n\\nDelimiters as Special Characters\\n\\nA delimiter could be any sequence of special characters that usually wouldn‚Äôt appear together, for example:\\n\\n###\\n\\n===\\n\\n>>>\\n\\nThe number and type of special characters chosen is inconsequential, as long as they are unique enough for the LLM to understand them as content separators instead of normal punctuation.\\n\\nHere‚Äôs an example of how you might use such delimiters in a prompt:\\n\\nClassify the sentiment of each conversation in <<<CONVERSATIONS>>> as \\n‚ÄòPositive‚Äô or ‚ÄòNegative‚Äô. Give the sentiment classifications without any other preamble text.\\n\\n###\\n\\nEXAMPLE CONVERSATIONS\\n\\n[Agent]: Good morning, how can I assist you today?\\n[Customer]: This product is terrible, nothing like what was advertised!\\n[Customer]: I‚Äôm extremely disappointed and expect a full refund.\\n\\n[Agent]: Good morning, how can I help you today?\\n[Customer]: Hi, I just wanted to say that I‚Äôm really impressed with your \\nproduct. It exceeded my expectations!\\n\\n###\\n\\nEXAMPLE OUTPUTS\\n\\nNegative\\n\\nPositive\\n\\n###\\n\\n<<<\\n[Agent]: Hello! Welcome to our support. How can I help you today?\\n[Customer]: Hi there! I just wanted to let you know I received my order, and \\nit‚Äôs fantastic!\\n[Agent]: That‚Äôs great to hear! We‚Äôre thrilled you‚Äôre happy with your purchase. \\nIs there anything else I can assist you with?\\n[Customer]: No, that‚Äôs it. Just wanted to give some positive feedback. Thanks \\nfor your excellent service!\\n\\n[Agent]: Hello, thank you for reaching out. How can I assist you today?\\n[Customer]: I‚Äôm very disappointed with my recent purchase. It‚Äôs not what I expected at all.\\n[Agent]: I‚Äôm sorry to hear that. Could you please provide more details so I can help?\\n[Customer]: The product is of poor quality and it arrived late. I‚Äôm really \\nunhappy with this experience.\\n>>>\\n\\nAbove, the examples are sectioned using the delimiter ###, with the section headings EXAMPLE CONVERSATIONS and EXAMPLE OUTPUTS in capital letters to differentiate them. The preamble states that the conversations to be classified are sectioned inside <<<CONVERSATIONS>>>, and these conversations are subsequently given to the LLM at the bottom of the prompt without any explanatory text, but the LLM understands that these are the conversations it should classify due to the presence of the delimiters <<< and >>>.\\n\\nHere is the output from GPT-4, with the sentiment classifications given without any other preamble text outputted, like what we asked for:\\n\\nPositive\\n\\nNegative\\n\\nDelimiters as XML Tags\\n\\nAnother approach to using delimiters is having them as XML tags. XML tags are tags enclosed in angle brackets, with opening and closing tags. An example is <tag> and </tag>. This is effective as LLMs have been trained on a lot of web content in XML, and have learned to understand its formatting.\\n\\nHere‚Äôs the same prompt above, but structured using XML tags as delimiters instead:\\n\\nClassify the sentiment of the following conversations into one of two classes, using the examples given. Give the sentiment classifications without any other\\npreamble text.\\n\\n<classes>\\nPositive\\nNegative\\n</classes>\\n\\n<example-conversations>\\n[Agent]: Good morning, how can I assist you today?\\n[Customer]: This product is terrible, nothing like what was advertised!\\n[Customer]: I‚Äôm extremely disappointed and expect a full refund.\\n\\n[Agent]: Good morning, how can I help you today?\\n[Customer]: Hi, I just wanted to say that I‚Äôm really impressed with your \\nproduct. It exceeded my expectations!\\n</example-conversations>\\n\\n<example-classes>\\nNegative\\n\\nPositive\\n</example-classes>\\n\\n<conversations>\\n[Agent]: Hello! Welcome to our support. How can I help you today?\\n[Customer]: Hi there! I just wanted to let you know I received my order, and \\nit‚Äôs fantastic!\\n[Agent]: That‚Äôs great to hear! We‚Äôre thrilled you‚Äôre happy with your purchase. \\nIs there anything else I can assist you with?\\n[Customer]: No, that‚Äôs it. Just wanted to give some positive feedback. Thanks \\nfor your excellent service!\\n\\n[Agent]: Hello, thank you for reaching out. How can I assist you today?\\n[Customer]: I‚Äôm very disappointed with my recent purchase. It‚Äôs not what I \\nexpected at all.\\n[Agent]: I‚Äôm sorry to hear that. Could you please provide more details so I \\ncan help?\\n[Customer]: The product is of poor quality and it arrived late. I‚Äôm really \\nunhappy with this experience.\\n</conversations>\\n\\nIt is beneficial to use the same noun for the XML tag as the words you have used to describe them in the instructions. The instructions we gave in the prompt above were:\\n\\nClassify the sentiment of the following conversations into one of two classes, using the examples given. Give the sentiment classifications without any other\\npreamble text.\\n\\nWhere we used the nouns conversations, classes, and examples. As such, the XML tags we use as delimiters are <conversations>, <classes>, <example-conversations>, and <example-classes>. This ensures that the LLM understands how your instructions relate to the XML tags used as delimiters.\\n\\nAgain, the sectioning of your instructions in a clear and structured manner through the use of delimiters ensures that GPT-4 responds exactly how you want it to:\\n\\nPositive\\n\\nNegative\\n\\n3. [üî¥] Creating System Prompts With LLM Guardrails\\n\\nBefore diving in, it is important to note that this section is relevant only to LLMs that possess a System Prompt feature, unlike the other sections in this article which are relevant for any LLM. The most notable LLM with this feature is, of course, ChatGPT, and therefore we will use ChatGPT as the illustrating example for this section.\\n\\nImage generated by DALL¬∑E 3\\n\\nTerminology surrounding System Prompts\\n\\nFirst, let‚Äôs iron out terminology: With regards to ChatGPT, there exists a plethora of resources using these 3 terms almost interchangeably: \"System Prompts\", \"System Messages\", and \"Custom Instructions\". This has proved confusing to many (including me!), so much so that OpenAI released an article explaining these terminologies. Here‚Äôs a quick summary of it:\\n\\n\"System Prompts\" and \"System Messages\" are terms used when interacting with ChatGPT programmatically over its Chat Completions API.\\n\\nOn the other hand, \"Custom Instructions\" is the term used when interacting with ChatGPT over its user interface at https://chat.openai.com/.\\n\\nImage from Enterprise DNA Blog\\n\\nOverall, though, the 3 terms refer to the same thing, so don‚Äôt let the terminology confuse you! Moving forward, this section will use the term \"System Prompts\". Now let‚Äôs dive in!\\n\\nWhat are System Prompts?\\n\\nSystem Prompts are an additional prompt where you provide instructions on how the LLM should behave. It is considered additional as it is outside of your \"normal\" prompts (better known as User Prompts) to the LLM.\\n\\nWithin a chat, every time you provide a new prompt, System Prompts act like a filter that the LLM automatically applies before giving its response to your new prompt. This means that the System Prompts are taken into account every time the LLM responds within the chat.\\n\\nWhen should System Prompts be used?\\n\\nThe first question on your mind might be: Why should I provide instructions inside the System Prompt when I can also provide them in my first prompt to a new chat, before further conversations with the LLM?\\n\\nThe answer is because LLMs have a limit to their conversational memory. In the latter case, as the conversation carries on, the LLM is likely to \"forget\" this first prompt you provided to the chat, making these instructions obsolete.\\n\\nOn the other hand, when instructions are provided in the System Prompt, these System Prompt instructions are automatically taken into account together with each new prompt provided to the chat. This ensures that the LLM continues to receive these instructions even as the conversation carries on, no matter how long the chat becomes.\\n\\nIn conclusion:\\n\\nUse System Prompts to provide instructions that you want the LLM to remember when responding throughout the entire chat.\\n\\nWhat should System Prompts include?\\n\\nInstructions in the System Prompt typically includes the following categories:\\n\\nTask definition, so the LLM will always remember what it has to do throughout the chat.\\n\\nOutput format, so the LLM will always remember how it should respond.\\n\\nGuardrails, so the LLM will always remember how it should *not* respond. Guardrails are emerging field in LLM governance, referring to configured boundaries that an LLM is allowed to operate in.\\n\\nFor example, a System Prompt might look like this:\\n\\nYou will answer questions using this text: [insert text]. \\nYou will respond with a JSON object in this format: {\"Question\": \"Answer\"}.\\nIf the text does not contain sufficient information to answer the question, do not make up information and give the answer as \"NA\". \\nYou are only allowed to answer questions related to [insert scope]. Never answer any questions related to demographic information such as age, gender, and religion.\\n\\nWhere each portion relates to the categories as follows:\\n\\nBreaking down a System Prompt - Image by author\\n\\nBut then what goes into the \"normal\" prompts to the chat?\\n\\nNow you might be thinking: That sounds like a lot of information already being given in the System Prompt. What do I put in my \"normal\" prompts (better known as User Prompts) to the chat then?\\n\\nThe System Prompt outlines the general task at hand. In the above System Prompt example, the task has been defined to only use a specific piece of text for question-answering, and the LLM is instructed to respond in the format {\"Question\": \"Answer\"}.\\n\\nYou will answer questions using this text: [insert text]. \\nYou will respond with a JSON object in this format: {\"Question\": \"Answer\"}.\\n\\nIn this case, each User Prompt to the chat would simply be the question that you want answered using the text. For example, a User Prompt might be \"What is the text about?\". And the LLM would respond with {\"What is the text about?\": \"The text is about...\"}.\\n\\nBut let‚Äôs generalize this task example further. In practice, it would be more likely that you have multiple pieces of text that you want to ask questions on, rather than just 1. In this case, we could edit the first line of the above System Prompt from\\n\\nYou will answer questions using this text: [insert text].\\n\\nto\\n\\nYou will answer questions using the provided text.\\n\\nNow, each User Prompt to the chat would include both the text to conduct question-answering over, and the question to be answered, such as:\\n\\n<text>\\n[insert text]\\n</text>\\n\\n<question>\\n[insert question]\\n</question>\\n\\nHere, we also use XML tags as delimiters in order to provide the 2 required pieces of information to the LLM in a structured manner. The nouns used in the XML tags, text and question, correspond to the nouns used in the System Prompt so that the LLM understands how the tags relate to the System Prompt instructions.\\n\\nIn conclusion, the System Prompt should give the overall task instructions, and each User Prompt should provide the exact specifics that you want the task to be executed using. In this case, for example, these exact specifics are the text and the question.\\n\\nExtra: Making LLM guardrails dynamic\\n\\nAbove, guardrails are added through a few sentences in the System Prompt. These guardrails are then set in stone and do not change for the entire chat. What if you wish to have different guardrails in place at different points of the conversation?\\n\\nUnfortunately for users of the ChatGPT user interface, there is no straightforward way to do this right now. However, if you‚Äôre interacting with ChatGPT programmatically, you‚Äôre in luck! The increasing focus on building effective LLM guardrails has seen the development of open-source packages that allow you to set up far more detailed and dynamic guardrails programmatically.\\n\\nA noteworthy one is NeMo Guardrails developed by the NVIDIA team, which allows you to configure the expected conversation flow between users and the LLM, and thus set up different guardrails at different points of the chat, allowing for dynamic guardrails that evolve as the chat progresses. I definitely recommend checking it out!\\n\\n4. [üî¥] Analyzing datasets using only LLMs, without plugins or code\\n\\nImage generated by DALL¬∑E 3\\n\\nYou might have heard of OpenAI‚Äôs Advanced Data Analysis plugin within ChatGPT‚Äôs GPT-4 that is available to premium (paid) accounts. It allows users to upload datasets to ChatGPT and run code directly on the dataset, allowing for accurate data analysis.\\n\\nBut did you know that you don‚Äôt always need such plugins to analyze datasets well with LLMs? Let‚Äôs first understand the strengths and limitations of purely using LLMs to analyze datasets.\\n\\nTypes of dataset analysis that LLMs are *not* great at\\n\\nAs you probably already know, LLMs are limited in their ability to perform accurate mathematical calculations, making them unsuitable for tasks requiring precise quantitative analysis on datasets, such as:\\n\\nDescriptive Statistics: Summarizing numerical columns quantitatively, through measures like the mean or variance.\\n\\nCorrelation Analysis: Obtaining the precise correlation coefficient between columns.\\n\\nStatistical Analysis: Such as hypothesis testing to determine if there are statistically significant differences between groups of data points.\\n\\nMachine Learning: Performing predictive modelling on a dataset such as using linear regressions, gradient boosted trees, or neural networks.\\n\\nPerforming such quantitative tasks on datasets is why OpenAI‚Äôs Advanced Data Analysis plugin exists, so that programming languages step in to run code for such tasks on a dataset.\\n\\nSo, why would anyone want to analyze datasets using only LLMs and without such plugins?\\n\\nTypes of dataset analysis that LLMs are great at\\n\\nLLMs are excellent at identifying patterns and trends. This capability stems from their extensive training on diverse and voluminous data, enabling them to discern intricate patterns that may not be immediately apparent.\\n\\nThis makes them well-suited for tasks based on pattern-finding within datasets, such as:\\n\\nAnomaly detection: Identifying unusual data points that deviate from the norm, based on one or more column values.\\n\\nClustering: Grouping data points with similar characteristics across columns.\\n\\nCross-Column Relationships: Identifying combined trends across columns.\\n\\nTextual Analysis (For text-based columns): Categorization based on topic or sentiment.\\n\\nTrend Analysis (For datasets with time aspects): Identifying patterns, seasonal variations, or trends within columns across time.\\n\\nFor such pattern-based tasks, using LLMs alone may in fact produce better results within a shorter timeframe than using code! Let‚Äôs illustrate this fully with an example.\\n\\nAnalyzing a Kaggle dataset using only LLMs\\n\\nWe‚Äôll use a popular real-world Kaggle dataset curated for Customer Personality Analysis, wherein a company seeks to segment its customer base in order to understand its customers better.\\n\\nFor easier validation of the LLM‚Äôs analysis later, we‚Äôll subset this dataset to 50 rows and retain only the most relevant columns. After which, the dataset for analysis looks like this, where each row represents a customer, and the columns depict customer information:\\n\\nFirst 3 rows of dataset - Image by author\\n\\nSay you work on the company‚Äôs marketing team. You are tasked to utilize this dataset of customer information to guide marketing efforts. This is a 2-step task: First, use the dataset to generate meaningful customer segments. Next, generate ideas on how to best market towards each segment. Now this is a practical business problem where the pattern-finding (for step 1) capability of LLMs can truly excel.\\n\\nLet‚Äôs craft a prompt for this task as follows, using 4 prompt engineering techniques (more on these later!):\\n1. Breaking down a complex task into simple steps\\n2. Referencing intermediate outputs from each step\\n3. Formatting the LLM‚Äôs response\\n4. Separating the instructions from the dataset\\n\\nSystem Prompt:\\nI want you to act as a data scientist to analyze datasets. Do not make up information that is not in the dataset. For each analysis I ask for, provide me with the exact and definitive answer and do not provide me with code or instructions to do the analysis on other platforms.\\n\\nPrompt:\\n# CONTEXT #\\nI sell wine. I have a dataset of information on my customers: [year of birth, marital status, income, number of children, days since last purchase, amount spent].\\n\\n#############\\n\\n# OBJECTIVE #\\nI want you use the dataset to cluster my customers into groups and then give me ideas on how to target my marketing efforts towards each group. Use this step-by-step process and do not use code:\\n\\n1. CLUSTERS: Use the columns of the dataset to cluster the rows of the dataset, such that customers within the same cluster have similar column values while customers in different clusters have distinctly different column values. Ensure that each row only belongs to 1 cluster.\\n \\nFor each cluster found,\\n2. CLUSTER_INFORMATION: Describe the cluster in terms of the dataset columns.\\n3. CLUSTER_NAME: Interpret [CLUSTER_INFORMATION] to obtain a short name for the customer group in this cluster.\\n4. MARKETING_IDEAS: Generate ideas to market my product to this customer group.\\n5. RATIONALE: Explain why [MARKETING_IDEAS] is relevant and effective for this customer group.\\n\\n#############\\n\\n# STYLE #\\nBusiness analytics report\\n\\n#############\\n\\n# TONE #\\nProfessional, technical\\n\\n#############\\n\\n# AUDIENCE #\\nMy business partners. Convince them that your marketing strategy is well thought-out and fully backed by data.\\n\\n#############\\n\\n# RESPONSE: MARKDOWN REPORT #\\n <For each cluster in [CLUSTERS]>\\n - Customer Group: [CLUSTER_NAME]\\n - Profile: [CLUSTER_INFORMATION]\\n - Marketing Ideas: [MARKETING_IDEAS]\\n - Rationale: [RATIONALE]\\n\\n<Annex>\\n Give a table of the list of row numbers belonging to each cluster, in order to back up your analysis. Use these table headers: [[CLUSTER_NAME], List of Rows].\\n\\n#############\\n\\n# START ANALYSIS #\\n If you understand, ask me for my dataset.\\n\\nBelow is GPT-4‚Äôs reply, and we proceed to pass the dataset to it in a CSV string.\\n\\nGPT-4\\'s response - Image by author\\n\\nFollowing which, GPT-4 replies with its analysis in the markdown report format we asked for:\\n\\nGPT-4\\'s response - Image by author\\n\\nGPT-4\\'s response - Image by author\\n\\nGPT-4\\'s response - Image by author\\n\\nValidating the LLM‚Äôs analysis\\n\\nFor the sake of brevity, we‚Äôll pick 2 customer groups generated by the LLM for validation - say, Young Families and Discerning Enthusiasts.\\n\\nYoung Families\\n- Profile synthesized by LLM: Born after 1980, Married or Together, Moderate to low income, Have children, Frequent small purchases.\\n- Rows clustered into this group by LLM: 3, 4, 7, 10, 16, 20\\n- Digging into the dataset, the full data for these rows are:\\n\\nFull data for Young Families - Image by author\\n\\nWhich exactly correspond to the profile identified by the LLM. It was even able to cluster the row with a null value without us preprocessing it beforehand!\\n\\nDiscerning Enthusiasts\\n- Profile synthesized by LLM: Wide age range, Any marital status, High income, Varied children status, High spend on purchases.\\n- Rows clustered into this group by LLM: 2, 5, 18, 29, 34, 36\\n- Digging into the dataset, the full data for these rows are:\\n\\nFull data for Discerning Enthusiasts - Image by author\\n\\nWhich again align very well with the profile identified by the LLM!\\n\\nThis example showcases LLMs‚Äô abilities in pattern-finding, interpreting and distilling multi-dimensional datasets into meaningful insights, while ensuring that its analysis is deeply rooted in the factual truth of the dataset.\\n\\nWhat if we used ChatGPT‚Äôs Advanced Data Analysis plugin?\\n\\nFor completeness, I attempted this same task with the same prompt, but asked ChatGPT to execute the analysis using code instead, which activated its Advanced Data Analysis plugin. The idea was for the plugin to run code using a clustering algorithm like K-Means directly on the dataset to obtain each customer group, before synthesizing the profile of each cluster to provide marketing strategies.\\n\\nHowever, multiple attempts resulted in the following error messages with no outputs, despite the dataset being only 50 rows:\\n\\nError and no output from Attempt 1 - Image by author\\n\\nError and no output from Attempt 2 - Image by author\\n\\nWith the Advanced Data Analysis plugin right now, it appears that executing simpler tasks on datasets such as calculating descriptive statistics or creating graphs can be easily achieved, but more advanced tasks that require computing of algorithms may sometimes result in errors and no outputs, due to computational limits or otherwise.\\n\\nSo‚Ä¶When to analyze datasets using LLMs?\\n\\nThe answer is it depends on the type of analysis.\\n\\nFor tasks requiring precise mathematical calculations or complex, rule-based processing, conventional programming methods remain superior.\\n\\nFor tasks based on pattern-recognition, it can be challenging or more time-consuming to execute using conventional programming and algorithmic approaches. LLMs, however, excel at such tasks, and can even provide additional outputs such as annexes to back up its analysis, and full analysis reports in markdown formatting.\\n\\nUltimately, the decision to utilize LLMs hinges on the nature of the task at hand, balancing the strengths of LLMs in pattern-recognition against the precision and specificity offered by traditional programming techniques.\\n\\nNow back to the prompt engineering!\\n\\nBefore this section ends, let‚Äôs go back to the prompt used to generate this dataset analysis and break down the key prompt engineering techniques used:\\n\\nPrompt:\\n# CONTEXT #\\nI sell wine. I have a dataset of information on my customers: [year of birth, marital status, income, number of children, days since last purchase, amount spent].\\n\\n#############\\n\\n# OBJECTIVE #\\nI want you use the dataset to cluster my customers into groups and then give me ideas on how to target my marketing efforts towards each group. Use this step-by-step process and do not use code:\\n\\n1. CLUSTERS: Use the columns of the dataset to cluster the rows of the dataset, such that customers within the same cluster have similar column values while customers in different clusters have distinctly different column values. Ensure that each row only belongs to 1 cluster.\\n \\nFor each cluster found,\\n2. CLUSTER_INFORMATION: Describe the cluster in terms of the dataset columns.\\n3. CLUSTER_NAME: Interpret [CLUSTER_INFORMATION] to obtain a short name for the customer group in this cluster.\\n4. MARKETING_IDEAS: Generate ideas to market my product to this customer group.\\n5. RATIONALE: Explain why [MARKETING_IDEAS] is relevant and effective for this customer group.\\n\\n#############\\n\\n# STYLE #\\nBusiness analytics report\\n\\n#############\\n\\n# TONE #\\nProfessional, technical\\n\\n#############\\n\\n# AUDIENCE #\\nMy business partners. Convince them that your marketing strategy is well thought-out and fully backed by data.\\n\\n#############\\n\\n# RESPONSE: MARKDOWN REPORT #\\n <For each cluster in [CLUSTERS]>\\n - Customer Group: [CLUSTER_NAME]\\n - Profile: [CLUSTER_INFORMATION]\\n - Marketing Ideas: [MARKETING_IDEAS]\\n - Rationale: [RATIONALE]\\n\\n<Annex>\\n Give a table of the list of row numbers belonging to each cluster, in order to back up your analysis. Use these table headers: [[CLUSTER_NAME], List of Rows].\\n\\n#############\\n\\n# START ANALYSIS #\\n If you understand, ask me for my dataset.\\n\\nTechnique 1: Breaking down a complex task into simple steps\\nLLMs are great at performing simple tasks, but not so great at complex ones. As such, with complex tasks like this one, it is important to break down the task into simple step-by-step instructions for the LLM to follow. The idea is to give the LLM the steps that you yourself would take to execute the task.\\n\\nIn this example, the steps are given as:\\n\\nUse this step-by-step process and do not use code:\\n\\n1. CLUSTERS: Use the columns of the dataset to cluster the rows of the dataset, such that customers within the same cluster have similar column values while customers in different clusters have distinctly different column values. Ensure that each row only belongs to 1 cluster.\\n \\nFor each cluster found,\\n2. CLUSTER_INFORMATION: Describe the cluster in terms of the dataset columns.\\n3. CLUSTER_NAME: Interpret [CLUSTER_INFORMATION] to obtain a short name for the customer group in this cluster.\\n4. MARKETING_IDEAS: Generate ideas to market my product to this customer group.\\n5. RATIONALE: Explain why [MARKETING_IDEAS] is relevant and effective for this customer group.\\n\\nAs opposed to simply giving the overall task to the LLM as \"Cluster the customers into groups and then give ideas on how to market to each group\".\\n\\nWith step-by-step instructions, LLMs are significantly more likely to deliver the correct results.\\n\\nTechnique 2: Referencing intermediate outputs from each step\\nWhen providing the step-by-step process to the LLM, we give the intermediate output from each step a capitalized VARIABLE_NAME, namely CLUSTERS, CLUSTER_INFORMATION, CLUSTER_NAME, MARKETING_IDEAS and RATIONALE.\\n\\nCapitalization is used to differentiate these variable names from the body of instructions given. These intermediate outputs can later be referenced using square brackets as [VARIABLE_NAME].\\n\\nTechnique 3: Formatting the LLM‚Äôs response\\nHere, we ask for a markdown report format, which beautifies the LLM‚Äôs response. Having variable names from intermediate outputs again comes in handy here to dictate the structure of the report.\\n\\n# RESPONSE: MARKDOWN REPORT #\\n <For each cluster in [CLUSTERS]>\\n - Customer Group: [CLUSTER_NAME]\\n - Profile: [CLUSTER_INFORMATION]\\n - Marketing Ideas: [MARKETING_IDEAS]\\n - Rationale: [RATIONALE]\\n\\n<Annex>\\n Give a table of the list of row numbers belonging to each cluster, in order to back up your analysis. Use these table headers: [[CLUSTER_NAME], List of Rows].\\n\\nIn fact, you could even subsequently ask ChatGPT to provide the report as a downloadable file, allowing you to work off of its response in writing your final report.\\n\\nSaving GPT-4\\'s response as a file - Image by author\\n\\nTechnique 4: Separating the task instructions from the dataset\\nYou‚Äôll notice that we never gave the dataset to the LLM in our first prompt. Instead, the prompt gives only the task instructions for the dataset analysis, with this added to the bottom:\\n\\n# START ANALYSIS #\\nIf you understand, ask me for my dataset.\\n\\nChatGPT then responded that it understands, and we passed the dataset to it as a CSV string in our next prompt:\\n\\nGPT-4\\'s response - Image by author\\n\\nBut why separate the instructions from the dataset?\\n\\nThe straightforward answer is that LLMs have a limit to their context window, or the number of tokens they can take as input in 1 prompt. A long prompt combining both instructions and data might exceed this limit, leading to truncation and loss of information.\\n\\nThe more intricate answer is that separating the instructions and the dataset helps the LLM maintain clarity in understanding each, with lower likelihood of missing out information. You might have experienced scenarios where the LLM \"accidentally forgets\" a certain instruction you gave as part of a longer prompt - for example, if you asked for a 100-word response and the LLM gives you a longer paragraph back. By receiving the instructions first, before the dataset that the instructions are for, the LLM can first digest what it should do, before executing it on the dataset provided next.\\n\\nNote however that this separation of instructions and dataset can only be achieved with chat LLMs as they maintain a conversational memory, unlike completion LLMs which do not.\\n\\nClosing Thoughts\\n\\nBefore this article ends, I wanted to share some personal reflections on this incredible journey.\\n\\nFirst, a heartfelt thank you to GovTech Singapore for orchestrating such an amazing competition.\\n\\nA live on-stage battle in the final round!\\n\\nSecond, a big shout-out to my fellow phenomenal competitors, who each brought something special, making the competition as enriching as it was challenging! I‚Äôll never forget the final round, with us battling it out on stage and a live audience cheering us on - an experience I‚Äôll always remember fondly.\\n\\nFor me, this wasn‚Äôt just a competition; it was a celebration of talent, creativity, and the spirit of learning. And I‚Äôm beyond excited to see what comes next!\\n\\nI had a lot of fun writing this, and if you had fun reading, I would really appreciate if you took a second to leave some claps and a follow! You can also buy me a matcha latte üçµ to fuel my next article :)\\n\\nSee you in the next one!\\nSheila'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract articles content\n",
    "articles['content'] = articles['content'].apply(lambda x: x['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute articles similar to a top article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/01 12:39:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initiate SparkSession\n",
    "spark = SparkSession.builder.appName(\"SimilarityApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2618x5355 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 811580 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Vectorize 'article content' using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', min_df=0.01, max_df=0.8)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(articles['content'].fillna(''))\n",
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.   , 0.085, 0.045, ..., 0.047, 0.029, 0.173],\n",
       "       [0.085, 1.   , 0.109, ..., 0.03 , 0.026, 0.044],\n",
       "       [0.045, 0.109, 1.   , ..., 0.027, 0.019, 0.019],\n",
       "       ...,\n",
       "       [0.047, 0.03 , 0.027, ..., 1.   , 0.569, 0.332],\n",
       "       [0.029, 0.026, 0.019, ..., 0.569, 1.   , 0.214],\n",
       "       [0.173, 0.044, 0.019, ..., 0.332, 0.214, 1.   ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 2: computer similarity scores for all articles\n",
    "top_article_similarity = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "top_article_similarity.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified get_similar_articles() by including scores\n",
    "\n",
    "# Step 3: Recommend Similar articles\n",
    "def get_similar_articles_with_scores(article_id, top_n=3):\n",
    "    article_idx = articles.index[articles['article_id'] == article_id].tolist()[0]\n",
    "    sim_scores = list(enumerate(top_article_similarity[article_idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:top_n+1]  # Exclude the article itself\n",
    "    similar_articles = [(articles['article_id'].iloc[i[0]], i[1]) for i in sim_scores]\n",
    "    return similar_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('34c195a93d41', 'a3c1ed861830', 0.38378177402087593),\n",
       " ('34c195a93d41', '62d1b61c6f02', 0.37921165381479305),\n",
       " ('34c195a93d41', '6ad21c4cfa99', 0.3661479627085197),\n",
       " ('8c339f8fb602', '29a7a970358f', 0.35649059059794547),\n",
       " ('8c339f8fb602', '444693429eea', 0.28352116024085955),\n",
       " ('8c339f8fb602', 'e30a01dc67f8', 0.2800230883355387)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate over all articles to get similar articles along with similarity scores\n",
    "similar_articles_data = []\n",
    "for article_id in articles['article_id']:\n",
    "    similar_articles = get_similar_articles_with_scores(article_id)\n",
    "    for sim_article_id, score in similar_articles:\n",
    "        similar_articles_data.append((article_id, sim_article_id, float(score)))\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"article_id\", StringType(), True),\n",
    "    StructField(\"similar_article_id\", StringType(), True),\n",
    "    StructField(\"similarity_score\", FloatType(), True)\n",
    "])\n",
    "\n",
    "similar_articles_data[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list to a Spark DataFrame\n",
    "similar_articles_df = spark.createDataFrame(similar_articles_data, schema=schema)\n",
    "\n",
    "# Now you can use SparkSQL to query this DataFrame\n",
    "similar_articles_df.createOrReplaceTempView(\"similar_articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+\n",
      "|similar_article_id|similarity_score|\n",
      "+------------------+----------------+\n",
      "|      a3c1ed861830|      0.38378176|\n",
      "|      62d1b61c6f02|      0.37921166|\n",
      "|      6ad21c4cfa99|      0.36614797|\n",
      "+------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "article_id = '34c195a93d41'  # Replace with the article ID you're interested in\n",
    "query = f\"\"\"\n",
    "SELECT similar_article_id, similarity_score\n",
    "FROM similar_articles\n",
    "WHERE article_id = '{article_id}'\n",
    "ORDER BY similarity_score DESC\n",
    "LIMIT 3\n",
    "\"\"\"\n",
    "top_similar_articles = spark.sql(query)\n",
    "top_similar_articles.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[article_id: string, similar_article_id: string, similarity_score: float]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_articles_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute similar writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "writers = writers_raw.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>_id</th>\n",
       "      <td>65e232c3b92836486ca7c091</td>\n",
       "      <td>65e232c3b92836486ca7c092</td>\n",
       "      <td>65e232c3b92836486ca7c093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>fca9db1c7da0</td>\n",
       "      <td>e10ad955760c</td>\n",
       "      <td>76398be9016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>username</th>\n",
       "      <td>sheilateozy</td>\n",
       "      <td>nikhiladithyan</td>\n",
       "      <td>machine-learning-made-simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fullname</th>\n",
       "      <td>Sheila Teo</td>\n",
       "      <td>Nikhil Adithyan</td>\n",
       "      <td>Devansh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bio</th>\n",
       "      <td>Data Scientist, https://www.linkedin.com/in/sh...</td>\n",
       "      <td>Founder @BacktestZone (https://www.backtestzon...</td>\n",
       "      <td>Writing about AI, Math, the Tech Industry and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>followers_count</th>\n",
       "      <td>2111</td>\n",
       "      <td>7044</td>\n",
       "      <td>13172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>following_count</th>\n",
       "      <td>16</td>\n",
       "      <td>41</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publication_following_count</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_url</th>\n",
       "      <td>https://miro.medium.com/1*UmlZGQsNhuv9kgQL6pFs...</td>\n",
       "      <td>https://miro.medium.com/1*fiFn4AhPBi-CG-cKxHk2...</td>\n",
       "      <td>https://miro.medium.com/1*xiFRgHfgfMR7S111UB2h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twitter_username</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Machine01776819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_writer_program_enrolled</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allow_notes</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium_member_at</th>\n",
       "      <td>2023-10-11 08:44:50</td>\n",
       "      <td>2024-01-03 15:38:18</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_suspended</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top_writer_in</th>\n",
       "      <td>[]</td>\n",
       "      <td>[artificial-intelligence, technology]</td>\n",
       "      <td>[artificial-intelligence, technology]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_list</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_book_author</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tipping_link</th>\n",
       "      <td>https://ko-fi.com/sheilateo</td>\n",
       "      <td>https://paypal.me/veerabadran75?country.x=IN&amp;l...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bg_image_url</th>\n",
       "      <td>https://miro.medium.com/1*UXNs5AmGon4xRSXANOKA...</td>\n",
       "      <td>https://miro.medium.com/1*NVIBpx9wZp0Xq9KkSPU5...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logo_image_url</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top_articles</th>\n",
       "      <td>[{'id': '34c195a93d41', 'title': 'How I Won Si...</td>\n",
       "      <td>[{'id': '0d11918ee0b3', 'title': 'Create a Sto...</td>\n",
       "      <td>[{'id': '5d39cff63d52', 'title': 'Understandin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             0  \\\n",
       "_id                                                   65e232c3b92836486ca7c091   \n",
       "id                                                                fca9db1c7da0   \n",
       "username                                                           sheilateozy   \n",
       "fullname                                                            Sheila Teo   \n",
       "bio                          Data Scientist, https://www.linkedin.com/in/sh...   \n",
       "followers_count                                                           2111   \n",
       "following_count                                                             16   \n",
       "publication_following_count                                                  2   \n",
       "image_url                    https://miro.medium.com/1*UmlZGQsNhuv9kgQL6pFs...   \n",
       "twitter_username                                                                 \n",
       "is_writer_program_enrolled                                                True   \n",
       "allow_notes                                                               True   \n",
       "medium_member_at                                           2023-10-11 08:44:50   \n",
       "is_suspended                                                             False   \n",
       "top_writer_in                                                               []   \n",
       "has_list                                                                 False   \n",
       "is_book_author                                                           False   \n",
       "tipping_link                                       https://ko-fi.com/sheilateo   \n",
       "bg_image_url                 https://miro.medium.com/1*UXNs5AmGon4xRSXANOKA...   \n",
       "logo_image_url                                                                   \n",
       "top_articles                 [{'id': '34c195a93d41', 'title': 'How I Won Si...   \n",
       "\n",
       "                                                                             1  \\\n",
       "_id                                                   65e232c3b92836486ca7c092   \n",
       "id                                                                e10ad955760c   \n",
       "username                                                        nikhiladithyan   \n",
       "fullname                                                       Nikhil Adithyan   \n",
       "bio                          Founder @BacktestZone (https://www.backtestzon...   \n",
       "followers_count                                                           7044   \n",
       "following_count                                                             41   \n",
       "publication_following_count                                                 11   \n",
       "image_url                    https://miro.medium.com/1*fiFn4AhPBi-CG-cKxHk2...   \n",
       "twitter_username                                                                 \n",
       "is_writer_program_enrolled                                                True   \n",
       "allow_notes                                                               True   \n",
       "medium_member_at                                           2024-01-03 15:38:18   \n",
       "is_suspended                                                             False   \n",
       "top_writer_in                            [artificial-intelligence, technology]   \n",
       "has_list                                                                  True   \n",
       "is_book_author                                                           False   \n",
       "tipping_link                 https://paypal.me/veerabadran75?country.x=IN&l...   \n",
       "bg_image_url                 https://miro.medium.com/1*NVIBpx9wZp0Xq9KkSPU5...   \n",
       "logo_image_url                                                                   \n",
       "top_articles                 [{'id': '0d11918ee0b3', 'title': 'Create a Sto...   \n",
       "\n",
       "                                                                             2  \n",
       "_id                                                   65e232c3b92836486ca7c093  \n",
       "id                                                                 76398be9016  \n",
       "username                                          machine-learning-made-simple  \n",
       "fullname                                                               Devansh  \n",
       "bio                          Writing about AI, Math, the Tech Industry and ...  \n",
       "followers_count                                                          13172  \n",
       "following_count                                                             21  \n",
       "publication_following_count                                                  2  \n",
       "image_url                    https://miro.medium.com/1*xiFRgHfgfMR7S111UB2h...  \n",
       "twitter_username                                               Machine01776819  \n",
       "is_writer_program_enrolled                                                True  \n",
       "allow_notes                                                               True  \n",
       "medium_member_at                                                                \n",
       "is_suspended                                                             False  \n",
       "top_writer_in                            [artificial-intelligence, technology]  \n",
       "has_list                                                                 False  \n",
       "is_book_author                                                           False  \n",
       "tipping_link                                                              None  \n",
       "bg_image_url                                                                    \n",
       "logo_image_url                                                                  \n",
       "top_articles                 [{'id': '5d39cff63d52', 'title': 'Understandin...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writers.head(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_article_info(top_articles):\n",
    "    \"\"\"\n",
    "    Concatenate information from top_articles for TF-IDF vectorization.\n",
    "\n",
    "    Parameters:\n",
    "    - top_articles: List of dictionaries, each representing an article.\n",
    "\n",
    "    Returns:\n",
    "    - concatenated_info: String, concatenated information of all articles.\n",
    "    \"\"\"\n",
    "    concatenated_info = \"\"\n",
    "\n",
    "    for article in top_articles:\n",
    "        # Extract information from each article\n",
    "        title = article.get('title', '')\n",
    "        subtitle = article.get('subtitle', '')\n",
    "        tags = ' '.join(article.get('tags', []))  # Convert list of tags to a space-separated string\n",
    "        topics = ' '.join(article.get('topics', []))  # Convert list of topics to a space-separated string\n",
    "        top_highlight = article.get('top_highlight', '')\n",
    "\n",
    "        # Concatenate article information, separated by spaces\n",
    "        article_info = f\"{title} {subtitle} {tags} {topics} {top_highlight}\"\n",
    "        concatenated_info += \" \" + article_info  # Add to the overall concatenated string\n",
    "\n",
    "    return concatenated_info.strip()  # Remove any leading/trailing whitespace\n",
    "\n",
    "# Assuming 'top_articles' is a column in your DataFrame 'df' that contains the list of article dictionaries\n",
    "writers['concatenated_info'] = writers['top_articles'].apply(concatenate_article_info)\n",
    "writers['concatenated_info'] = writers['concatenated_info'] +\" \"+ writers['bio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How I Won Singapore‚Äôs GPT-4 Prompt Engineering Competition A deep dive into the strategies I learned for harnessing the power of Large Language Models (LLMs) data-science artificial-intelligence prompt-engineering editors-pick technology artificial-intelligence data-science Use System Prompts to provide instructions that you want the LLM to remember when responding throughout the entire chat. Stacked Ensembles for Advanced Predictive Modeling With H2O.ai and Optuna And how I placed top 10% in Europe‚Äôs largest machine learning competition with them! machine-learning data-science deep-learning ensemble-learning python machine-learning data-science Data Scientist, https://www.linkedin.com/in/sheila-teo/'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writers['concatenated_info'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_averages_and_proportions(top_articles):\n",
    "    # Initialize sums and counts for each metric\n",
    "    sums = {\n",
    "        'claps': 0,\n",
    "        'voters': 0,\n",
    "        'word_count': 0,\n",
    "        'responses_count': 0,\n",
    "        'reading_time': 0,\n",
    "        'is_series_count': 0,\n",
    "        'is_shortform_count': 0\n",
    "    }\n",
    "    count = len(top_articles)  # Number of articles to average over\n",
    "    \n",
    "    for article in top_articles:\n",
    "        # Sum up each metric\n",
    "        sums['claps'] += article.get('claps', 0)\n",
    "        sums['voters'] += article.get('voters', 0)\n",
    "        sums['word_count'] += article.get('word_count', 0)\n",
    "        sums['responses_count'] += article.get('responses_count', 0)\n",
    "        sums['reading_time'] += article.get('reading_time', 0)\n",
    "    \n",
    "    # Calculate averages and proportions\n",
    "    averages_and_proportions = {\n",
    "        'avg_claps': sums['claps'] / count if count else 0,\n",
    "        'avg_voters': sums['voters'] / count if count else 0,\n",
    "        'avg_word_count': sums['word_count'] / count if count else 0,\n",
    "        'avg_responses_count': sums['responses_count'] / count if count else 0,\n",
    "        'avg_reading_time': sums['reading_time'] / count if count else 0,\n",
    "    }\n",
    "    \n",
    "    return averages_and_proportions\n",
    "\n",
    "# Apply the function to each row in the DataFrame and expand the results into new columns\n",
    "for column, default in [('avg_claps', 0), ('avg_voters', 0), ('avg_word_count', 0), \n",
    "                        ('avg_responses_count', 0), ('avg_reading_time', 0)]:\n",
    "    writers[column] = writers['top_articles'].apply(lambda x: compute_averages_and_proportions(x).get(column, default))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>fca9db1c7da0</td>\n",
       "      <td>e10ad955760c</td>\n",
       "      <td>76398be9016</td>\n",
       "      <td>d80580992695</td>\n",
       "      <td>8a910484fe84</td>\n",
       "      <td>4beacba7dc8a</td>\n",
       "      <td>630ab5ffdf27</td>\n",
       "      <td>b856005e5ecd</td>\n",
       "      <td>b0fbe613be9d</td>\n",
       "      <td>14176fcb5743</td>\n",
       "      <td>...</td>\n",
       "      <td>8c8e5b7182ef</td>\n",
       "      <td>37a2cbe8bd15</td>\n",
       "      <td>15a29a4fc6ad</td>\n",
       "      <td>fb44e21903f3</td>\n",
       "      <td>c24a3d106811</td>\n",
       "      <td>c3aeaf49d8a4</td>\n",
       "      <td>b1a64eb107f0</td>\n",
       "      <td>c4a298b66f16</td>\n",
       "      <td>86f03cf61226</td>\n",
       "      <td>aff72a0c1243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>username</th>\n",
       "      <td>sheilateozy</td>\n",
       "      <td>nikhiladithyan</td>\n",
       "      <td>machine-learning-made-simple</td>\n",
       "      <td>anmol3015</td>\n",
       "      <td>moneytent</td>\n",
       "      <td>jordan_gibbs</td>\n",
       "      <td>jacobistyping</td>\n",
       "      <td>fareedkhandev</td>\n",
       "      <td>cobusgreyling</td>\n",
       "      <td>inchristiely</td>\n",
       "      <td>...</td>\n",
       "      <td>iampaulrose</td>\n",
       "      <td>pareto_investor</td>\n",
       "      <td>miptgirl</td>\n",
       "      <td>frank-andrade</td>\n",
       "      <td>cristianleo120</td>\n",
       "      <td>netflixtechblog</td>\n",
       "      <td>evertongomede</td>\n",
       "      <td>austin-starks</td>\n",
       "      <td>amankharwal</td>\n",
       "      <td>sh-tsang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fullname</th>\n",
       "      <td>Sheila Teo</td>\n",
       "      <td>Nikhil Adithyan</td>\n",
       "      <td>Devansh</td>\n",
       "      <td>Anmol Tomar</td>\n",
       "      <td>Money Tent</td>\n",
       "      <td>Jordan Gibbs</td>\n",
       "      <td>Jacob Bennett</td>\n",
       "      <td>Fareed Khan</td>\n",
       "      <td>Cobus Greyling</td>\n",
       "      <td>Christie C.</td>\n",
       "      <td>...</td>\n",
       "      <td>Paul Rose</td>\n",
       "      <td>The Pareto Investor</td>\n",
       "      <td>Mariya Mansurova</td>\n",
       "      <td>The PyCoach</td>\n",
       "      <td>Cristian Leo</td>\n",
       "      <td>Netflix Technology Blog</td>\n",
       "      <td>Everton Gomede, PhD</td>\n",
       "      <td>Austin Starks</td>\n",
       "      <td>Aman Kharwal</td>\n",
       "      <td>Sik-Ho Tsang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>followers_count</th>\n",
       "      <td>2111</td>\n",
       "      <td>7044</td>\n",
       "      <td>13172</td>\n",
       "      <td>19854</td>\n",
       "      <td>3516</td>\n",
       "      <td>2825</td>\n",
       "      <td>25716</td>\n",
       "      <td>20638</td>\n",
       "      <td>16025</td>\n",
       "      <td>56048</td>\n",
       "      <td>...</td>\n",
       "      <td>40019</td>\n",
       "      <td>38872</td>\n",
       "      <td>7465</td>\n",
       "      <td>135695</td>\n",
       "      <td>5804</td>\n",
       "      <td>404081</td>\n",
       "      <td>13191</td>\n",
       "      <td>4106</td>\n",
       "      <td>21174</td>\n",
       "      <td>11817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_book_author</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concatenated_info</th>\n",
       "      <td>How I Won Singapore‚Äôs GPT-4 Prompt Engineering...</td>\n",
       "      <td>Create a Stock Chatbot with your own CSV Data ...</td>\n",
       "      <td>Understanding Google‚Äôs GPT Killer- The Pathway...</td>\n",
       "      <td>Don‚Äôt use loc/iloc with Loops In Python, Inste...</td>\n",
       "      <td>Want to be Rich? DON‚ÄôT Start a Side Hustle. Wh...</td>\n",
       "      <td>Forget Prompt Engineering, ChatGPT Can Write P...</td>\n",
       "      <td>My magical first job as a self-taught software...</td>\n",
       "      <td>100x Faster‚Ää‚Äî‚ÄäScaling Your RAG App for Billion...</td>\n",
       "      <td>Demonstrate, Search, Predict (DSP) for LLMs Th...</td>\n",
       "      <td>Midjourney V6 New Prompting Technique‚Ää‚Äî‚ÄäIntrod...</td>\n",
       "      <td>...</td>\n",
       "      <td>I Built The 5 Income Streams Every Writer Shou...</td>\n",
       "      <td>Why You Should Pay Attention to Perplexity AI ...</td>\n",
       "      <td>Text Embeddings: Comprehensive Guide Evolution...</td>\n",
       "      <td>You‚Äôre Not The Only One Feeling AI Fatigue (Or...</td>\n",
       "      <td>Reinforcement Learning 101: Q-Learning Decodin...</td>\n",
       "      <td>Announcing bpftop: Streamlining eBPF performan...</td>\n",
       "      <td>Recommended BooksüßêüÜï(update at 2024‚Äì02‚Äì29) I wi...</td>\n",
       "      <td>I am Aurora, the Most Powerful AI Financial As...</td>\n",
       "      <td>85+ Data Science Projects You Can Try with Pyt...</td>\n",
       "      <td>Summary: My Paper Reading Lists, Tutorials &amp; S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_claps</th>\n",
       "      <td>5934.0</td>\n",
       "      <td>680.5</td>\n",
       "      <td>256.3</td>\n",
       "      <td>998.6</td>\n",
       "      <td>43.6</td>\n",
       "      <td>1201.8</td>\n",
       "      <td>1732.2</td>\n",
       "      <td>715.0</td>\n",
       "      <td>180.1</td>\n",
       "      <td>856.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1266.9</td>\n",
       "      <td>890.3</td>\n",
       "      <td>778.6</td>\n",
       "      <td>825.6</td>\n",
       "      <td>609.9</td>\n",
       "      <td>696.1</td>\n",
       "      <td>43.9</td>\n",
       "      <td>138.9</td>\n",
       "      <td>327.7</td>\n",
       "      <td>158.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_voters</th>\n",
       "      <td>1202.0</td>\n",
       "      <td>161.9</td>\n",
       "      <td>48.8</td>\n",
       "      <td>276.9</td>\n",
       "      <td>5.9</td>\n",
       "      <td>213.7</td>\n",
       "      <td>462.0</td>\n",
       "      <td>143.5</td>\n",
       "      <td>19.4</td>\n",
       "      <td>108.7</td>\n",
       "      <td>...</td>\n",
       "      <td>148.6</td>\n",
       "      <td>147.4</td>\n",
       "      <td>168.1</td>\n",
       "      <td>149.2</td>\n",
       "      <td>152.7</td>\n",
       "      <td>189.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>14.8</td>\n",
       "      <td>113.6</td>\n",
       "      <td>26.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_word_count</th>\n",
       "      <td>4352.5</td>\n",
       "      <td>2127.1</td>\n",
       "      <td>2117.3</td>\n",
       "      <td>891.9</td>\n",
       "      <td>787.8</td>\n",
       "      <td>1247.1</td>\n",
       "      <td>969.9</td>\n",
       "      <td>3552.4</td>\n",
       "      <td>1018.5</td>\n",
       "      <td>1393.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1878.6</td>\n",
       "      <td>829.4</td>\n",
       "      <td>4636.3</td>\n",
       "      <td>1007.3</td>\n",
       "      <td>4090.7</td>\n",
       "      <td>1636.3</td>\n",
       "      <td>1274.5</td>\n",
       "      <td>1370.4</td>\n",
       "      <td>662.6</td>\n",
       "      <td>859.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_responses_count</th>\n",
       "      <td>84.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>13.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>17.8</td>\n",
       "      <td>21.1</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>10.8</td>\n",
       "      <td>...</td>\n",
       "      <td>29.8</td>\n",
       "      <td>16.4</td>\n",
       "      <td>10.8</td>\n",
       "      <td>11.7</td>\n",
       "      <td>5.7</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_reading_time</th>\n",
       "      <td>17.549528</td>\n",
       "      <td>9.175126</td>\n",
       "      <td>8.964811</td>\n",
       "      <td>4.207327</td>\n",
       "      <td>3.356164</td>\n",
       "      <td>5.561038</td>\n",
       "      <td>4.085</td>\n",
       "      <td>14.81195</td>\n",
       "      <td>4.940063</td>\n",
       "      <td>7.086981</td>\n",
       "      <td>...</td>\n",
       "      <td>8.914057</td>\n",
       "      <td>3.674811</td>\n",
       "      <td>18.988805</td>\n",
       "      <td>4.911132</td>\n",
       "      <td>16.519937</td>\n",
       "      <td>6.774717</td>\n",
       "      <td>5.317767</td>\n",
       "      <td>6.029654</td>\n",
       "      <td>2.700377</td>\n",
       "      <td>4.136509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows √ó 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    0   \\\n",
       "id                                                        fca9db1c7da0   \n",
       "username                                                   sheilateozy   \n",
       "fullname                                                    Sheila Teo   \n",
       "followers_count                                                   2111   \n",
       "is_book_author                                                   False   \n",
       "concatenated_info    How I Won Singapore‚Äôs GPT-4 Prompt Engineering...   \n",
       "avg_claps                                                       5934.0   \n",
       "avg_voters                                                      1202.0   \n",
       "avg_word_count                                                  4352.5   \n",
       "avg_responses_count                                               84.0   \n",
       "avg_reading_time                                             17.549528   \n",
       "\n",
       "                                                                    1   \\\n",
       "id                                                        e10ad955760c   \n",
       "username                                                nikhiladithyan   \n",
       "fullname                                               Nikhil Adithyan   \n",
       "followers_count                                                   7044   \n",
       "is_book_author                                                   False   \n",
       "concatenated_info    Create a Stock Chatbot with your own CSV Data ...   \n",
       "avg_claps                                                        680.5   \n",
       "avg_voters                                                       161.9   \n",
       "avg_word_count                                                  2127.1   \n",
       "avg_responses_count                                               10.0   \n",
       "avg_reading_time                                              9.175126   \n",
       "\n",
       "                                                                    2   \\\n",
       "id                                                         76398be9016   \n",
       "username                                  machine-learning-made-simple   \n",
       "fullname                                                       Devansh   \n",
       "followers_count                                                  13172   \n",
       "is_book_author                                                   False   \n",
       "concatenated_info    Understanding Google‚Äôs GPT Killer- The Pathway...   \n",
       "avg_claps                                                        256.3   \n",
       "avg_voters                                                        48.8   \n",
       "avg_word_count                                                  2117.3   \n",
       "avg_responses_count                                                2.1   \n",
       "avg_reading_time                                              8.964811   \n",
       "\n",
       "                                                                    3   \\\n",
       "id                                                        d80580992695   \n",
       "username                                                     anmol3015   \n",
       "fullname                                                   Anmol Tomar   \n",
       "followers_count                                                  19854   \n",
       "is_book_author                                                   False   \n",
       "concatenated_info    Don‚Äôt use loc/iloc with Loops In Python, Inste...   \n",
       "avg_claps                                                        998.6   \n",
       "avg_voters                                                       276.9   \n",
       "avg_word_count                                                   891.9   \n",
       "avg_responses_count                                               13.3   \n",
       "avg_reading_time                                              4.207327   \n",
       "\n",
       "                                                                    4   \\\n",
       "id                                                        8a910484fe84   \n",
       "username                                                     moneytent   \n",
       "fullname                                                    Money Tent   \n",
       "followers_count                                                   3516   \n",
       "is_book_author                                                   False   \n",
       "concatenated_info    Want to be Rich? DON‚ÄôT Start a Side Hustle. Wh...   \n",
       "avg_claps                                                         43.6   \n",
       "avg_voters                                                         5.9   \n",
       "avg_word_count                                                   787.8   \n",
       "avg_responses_count                                                0.9   \n",
       "avg_reading_time                                              3.356164   \n",
       "\n",
       "                                                                    5   \\\n",
       "id                                                        4beacba7dc8a   \n",
       "username                                                  jordan_gibbs   \n",
       "fullname                                                  Jordan Gibbs   \n",
       "followers_count                                                   2825   \n",
       "is_book_author                                                   False   \n",
       "concatenated_info    Forget Prompt Engineering, ChatGPT Can Write P...   \n",
       "avg_claps                                                       1201.8   \n",
       "avg_voters                                                       213.7   \n",
       "avg_word_count                                                  1247.1   \n",
       "avg_responses_count                                               17.8   \n",
       "avg_reading_time                                              5.561038   \n",
       "\n",
       "                                                                    6   \\\n",
       "id                                                        630ab5ffdf27   \n",
       "username                                                 jacobistyping   \n",
       "fullname                                                 Jacob Bennett   \n",
       "followers_count                                                  25716   \n",
       "is_book_author                                                   False   \n",
       "concatenated_info    My magical first job as a self-taught software...   \n",
       "avg_claps                                                       1732.2   \n",
       "avg_voters                                                       462.0   \n",
       "avg_word_count                                                   969.9   \n",
       "avg_responses_count                                               21.1   \n",
       "avg_reading_time                                                 4.085   \n",
       "\n",
       "                                                                    7   \\\n",
       "id                                                        b856005e5ecd   \n",
       "username                                                 fareedkhandev   \n",
       "fullname                                                   Fareed Khan   \n",
       "followers_count                                                  20638   \n",
       "is_book_author                                                   False   \n",
       "concatenated_info    100x Faster‚Ää‚Äî‚ÄäScaling Your RAG App for Billion...   \n",
       "avg_claps                                                        715.0   \n",
       "avg_voters                                                       143.5   \n",
       "avg_word_count                                                  3552.4   \n",
       "avg_responses_count                                                8.5   \n",
       "avg_reading_time                                              14.81195   \n",
       "\n",
       "                                                                    8   \\\n",
       "id                                                        b0fbe613be9d   \n",
       "username                                                 cobusgreyling   \n",
       "fullname                                                Cobus Greyling   \n",
       "followers_count                                                  16025   \n",
       "is_book_author                                                   False   \n",
       "concatenated_info    Demonstrate, Search, Predict (DSP) for LLMs Th...   \n",
       "avg_claps                                                        180.1   \n",
       "avg_voters                                                        19.4   \n",
       "avg_word_count                                                  1018.5   \n",
       "avg_responses_count                                                0.6   \n",
       "avg_reading_time                                              4.940063   \n",
       "\n",
       "                                                                    9   ...  \\\n",
       "id                                                        14176fcb5743  ...   \n",
       "username                                                  inchristiely  ...   \n",
       "fullname                                                   Christie C.  ...   \n",
       "followers_count                                                  56048  ...   \n",
       "is_book_author                                                   False  ...   \n",
       "concatenated_info    Midjourney V6 New Prompting Technique‚Ää‚Äî‚ÄäIntrod...  ...   \n",
       "avg_claps                                                        856.6  ...   \n",
       "avg_voters                                                       108.7  ...   \n",
       "avg_word_count                                                  1393.1  ...   \n",
       "avg_responses_count                                               10.8  ...   \n",
       "avg_reading_time                                              7.086981  ...   \n",
       "\n",
       "                                                                    12  \\\n",
       "id                                                        8c8e5b7182ef   \n",
       "username                                                   iampaulrose   \n",
       "fullname                                                     Paul Rose   \n",
       "followers_count                                                  40019   \n",
       "is_book_author                                                   False   \n",
       "concatenated_info    I Built The 5 Income Streams Every Writer Shou...   \n",
       "avg_claps                                                       1266.9   \n",
       "avg_voters                                                       148.6   \n",
       "avg_word_count                                                  1878.6   \n",
       "avg_responses_count                                               29.8   \n",
       "avg_reading_time                                              8.914057   \n",
       "\n",
       "                                                                    13  \\\n",
       "id                                                        37a2cbe8bd15   \n",
       "username                                               pareto_investor   \n",
       "fullname                                           The Pareto Investor   \n",
       "followers_count                                                  38872   \n",
       "is_book_author                                                    True   \n",
       "concatenated_info    Why You Should Pay Attention to Perplexity AI ...   \n",
       "avg_claps                                                        890.3   \n",
       "avg_voters                                                       147.4   \n",
       "avg_word_count                                                   829.4   \n",
       "avg_responses_count                                               16.4   \n",
       "avg_reading_time                                              3.674811   \n",
       "\n",
       "                                                                    14  \\\n",
       "id                                                        15a29a4fc6ad   \n",
       "username                                                      miptgirl   \n",
       "fullname                                              Mariya Mansurova   \n",
       "followers_count                                                   7465   \n",
       "is_book_author                                                   False   \n",
       "concatenated_info    Text Embeddings: Comprehensive Guide Evolution...   \n",
       "avg_claps                                                        778.6   \n",
       "avg_voters                                                       168.1   \n",
       "avg_word_count                                                  4636.3   \n",
       "avg_responses_count                                               10.8   \n",
       "avg_reading_time                                             18.988805   \n",
       "\n",
       "                                                                    15  \\\n",
       "id                                                        fb44e21903f3   \n",
       "username                                                 frank-andrade   \n",
       "fullname                                                   The PyCoach   \n",
       "followers_count                                                 135695   \n",
       "is_book_author                                                   False   \n",
       "concatenated_info    You‚Äôre Not The Only One Feeling AI Fatigue (Or...   \n",
       "avg_claps                                                        825.6   \n",
       "avg_voters                                                       149.2   \n",
       "avg_word_count                                                  1007.3   \n",
       "avg_responses_count                                               11.7   \n",
       "avg_reading_time                                              4.911132   \n",
       "\n",
       "                                                                    16  \\\n",
       "id                                                        c24a3d106811   \n",
       "username                                                cristianleo120   \n",
       "fullname                                                  Cristian Leo   \n",
       "followers_count                                                   5804   \n",
       "is_book_author                                                   False   \n",
       "concatenated_info    Reinforcement Learning 101: Q-Learning Decodin...   \n",
       "avg_claps                                                        609.9   \n",
       "avg_voters                                                       152.7   \n",
       "avg_word_count                                                  4090.7   \n",
       "avg_responses_count                                                5.7   \n",
       "avg_reading_time                                             16.519937   \n",
       "\n",
       "                                                                    17  \\\n",
       "id                                                        c3aeaf49d8a4   \n",
       "username                                               netflixtechblog   \n",
       "fullname                                       Netflix Technology Blog   \n",
       "followers_count                                                 404081   \n",
       "is_book_author                                                   False   \n",
       "concatenated_info    Announcing bpftop: Streamlining eBPF performan...   \n",
       "avg_claps                                                        696.1   \n",
       "avg_voters                                                       189.8   \n",
       "avg_word_count                                                  1636.3   \n",
       "avg_responses_count                                                9.9   \n",
       "avg_reading_time                                              6.774717   \n",
       "\n",
       "                                                                    18  \\\n",
       "id                                                        b1a64eb107f0   \n",
       "username                                                 evertongomede   \n",
       "fullname                                           Everton Gomede, PhD   \n",
       "followers_count                                                  13191   \n",
       "is_book_author                                                    True   \n",
       "concatenated_info    Recommended BooksüßêüÜï(update at 2024‚Äì02‚Äì29) I wi...   \n",
       "avg_claps                                                         43.9   \n",
       "avg_voters                                                         6.1   \n",
       "avg_word_count                                                  1274.5   \n",
       "avg_responses_count                                                0.1   \n",
       "avg_reading_time                                              5.317767   \n",
       "\n",
       "                                                                    19  \\\n",
       "id                                                        c4a298b66f16   \n",
       "username                                                 austin-starks   \n",
       "fullname                                                 Austin Starks   \n",
       "followers_count                                                   4106   \n",
       "is_book_author                                                   False   \n",
       "concatenated_info    I am Aurora, the Most Powerful AI Financial As...   \n",
       "avg_claps                                                        138.9   \n",
       "avg_voters                                                        14.8   \n",
       "avg_word_count                                                  1370.4   \n",
       "avg_responses_count                                                1.4   \n",
       "avg_reading_time                                              6.029654   \n",
       "\n",
       "                                                                    20  \\\n",
       "id                                                        86f03cf61226   \n",
       "username                                                   amankharwal   \n",
       "fullname                                                  Aman Kharwal   \n",
       "followers_count                                                  21174   \n",
       "is_book_author                                                    True   \n",
       "concatenated_info    85+ Data Science Projects You Can Try with Pyt...   \n",
       "avg_claps                                                        327.7   \n",
       "avg_voters                                                       113.6   \n",
       "avg_word_count                                                   662.6   \n",
       "avg_responses_count                                                3.0   \n",
       "avg_reading_time                                              2.700377   \n",
       "\n",
       "                                                                    21  \n",
       "id                                                        aff72a0c1243  \n",
       "username                                                      sh-tsang  \n",
       "fullname                                                  Sik-Ho Tsang  \n",
       "followers_count                                                  11817  \n",
       "is_book_author                                                   False  \n",
       "concatenated_info    Summary: My Paper Reading Lists, Tutorials & S...  \n",
       "avg_claps                                                        158.3  \n",
       "avg_voters                                                        26.1  \n",
       "avg_word_count                                                   859.0  \n",
       "avg_responses_count                                                1.4  \n",
       "avg_reading_time                                              4.136509  \n",
       "\n",
       "[11 rows x 22 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming `df` is your DataFrame\n",
    "writers = writers.drop(columns=['_id', 'bio', 'following_count',\n",
    "                      'publication_following_count', 'image_url', 'twitter_username', 'is_writer_program_enrolled',\n",
    "                      'allow_notes', 'medium_member_at', 'is_suspended', 'top_writer_in', 'has_list',\n",
    "                      'tipping_link', 'bg_image_url', 'logo_image_url', 'top_articles'])\n",
    "writers.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "# Fit and transform the 'concatenated_info' column\n",
    "tfidf_writer_matrix = tfidf_vectorizer.fit_transform(writers['concatenated_info'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now encode the followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     fca9db1c7da0\n",
       "1     e10ad955760c\n",
       "2      76398be9016\n",
       "3     d80580992695\n",
       "4     8a910484fe84\n",
       "5     4beacba7dc8a\n",
       "6     630ab5ffdf27\n",
       "7     b856005e5ecd\n",
       "8     b0fbe613be9d\n",
       "9     14176fcb5743\n",
       "10    9b351e8113e9\n",
       "11    5d33decdf4c4\n",
       "12    8c8e5b7182ef\n",
       "13    37a2cbe8bd15\n",
       "14    15a29a4fc6ad\n",
       "15    fb44e21903f3\n",
       "16    c24a3d106811\n",
       "17    c3aeaf49d8a4\n",
       "18    b1a64eb107f0\n",
       "19    c4a298b66f16\n",
       "20    86f03cf61226\n",
       "21    aff72a0c1243\n",
       "Name: id, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_writers = writers['id']\n",
    "all_writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<68398x22 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 99647 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a mapping of writer IDs to column indices for efficient lookup\n",
    "writer_to_index = {writer: index for index, writer in enumerate(all_writers)}\n",
    "\n",
    "# Initialize a list to hold the data for the sparse matrix\n",
    "rows, cols, data = [], [], []\n",
    "\n",
    "# Iterate over the DataFrame to populate rows, cols, and data\n",
    "for row_idx, writer_ids in enumerate(followers['writer_ids']):\n",
    "    for writer_id in writer_ids:\n",
    "        # Check if the writer_id is in the mapping\n",
    "        if writer_id in writer_to_index:\n",
    "            col_idx = writer_to_index[writer_id]\n",
    "            rows.append(row_idx)\n",
    "            cols.append(col_idx)\n",
    "            data.append(1)  # This is a binary matrix, so we just set the value to 1\n",
    "\n",
    "# Create the sparse matrix directly from the rows, cols, and data\n",
    "writer_sparse_matrix = sparse.csr_matrix((data, (rows, cols)), shape=(len(followers), len(all_writers)))\n",
    "writer_sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<22x68398 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 99647 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "follower_sparse_matrix = writer_sparse_matrix.transpose()\n",
    "follower_sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'is_book_author' from boolean to int\n",
    "writers['is_book_author'] = writers['is_book_author'].astype(int)\n",
    "\n",
    "# Select numerical features\n",
    "numerical_features = writers[['avg_claps', 'avg_voters', 'avg_word_count', 'avg_responses_count', \n",
    "                              'avg_reading_time', 'followers_count', 'is_book_author']].values\n",
    "\n",
    "# Normalize numerical features\n",
    "normalized_numerical_features = normalize(numerical_features, axis=0)\n",
    "\n",
    "# Convert to sparse matrix\n",
    "numerical_features_sparse = csr_matrix(normalized_numerical_features)\n",
    "\n",
    "# Assuming 'tfidf_matrix' is the TF-IDF sparse matrix and 'followers_matrix' is the followers sparse matrix\n",
    "final_matrix = hstack([tfidf_writer_matrix, follower_sparse_matrix, numerical_features_sparse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.   , 0.016, 0.017, 0.016, 0.031, 0.097, 0.01 , 0.015, 0.024,\n",
       "        0.007, 0.008, 0.015, 0.009, 0.007, 0.033, 0.005, 0.02 , 0.001,\n",
       "        0.01 , 0.028, 0.005, 0.011],\n",
       "       [0.016, 1.   , 0.091, 0.121, 0.023, 0.046, 0.111, 0.054, 0.061,\n",
       "        0.022, 0.036, 0.067, 0.027, 0.056, 0.147, 0.023, 0.17 , 0.003,\n",
       "        0.148, 0.114, 0.013, 0.038],\n",
       "       [0.017, 0.091, 1.   , 0.123, 0.016, 0.036, 0.046, 0.114, 0.135,\n",
       "        0.028, 0.05 , 0.161, 0.047, 0.052, 0.081, 0.05 , 0.048, 0.009,\n",
       "        0.135, 0.07 , 0.048, 0.091],\n",
       "       [0.016, 0.121, 0.123, 1.   , 0.029, 0.038, 0.15 , 0.272, 0.102,\n",
       "        0.082, 0.12 , 0.203, 0.075, 0.149, 0.109, 0.061, 0.069, 0.009,\n",
       "        0.211, 0.041, 0.065, 0.051],\n",
       "       [0.031, 0.023, 0.016, 0.029, 1.   , 0.069, 0.007, 0.035, 0.029,\n",
       "        0.016, 0.02 , 0.01 , 0.028, 0.02 , 0.047, 0.009, 0.013, 0.002,\n",
       "        0.037, 0.054, 0.002, 0.009],\n",
       "       [0.097, 0.046, 0.036, 0.038, 0.069, 1.   , 0.025, 0.044, 0.046,\n",
       "        0.024, 0.024, 0.028, 0.024, 0.029, 0.05 , 0.011, 0.028, 0.001,\n",
       "        0.048, 0.067, 0.003, 0.019],\n",
       "       [0.01 , 0.111, 0.046, 0.15 , 0.007, 0.025, 1.   , 0.058, 0.04 ,\n",
       "        0.056, 0.116, 0.106, 0.048, 0.118, 0.058, 0.082, 0.067, 0.006,\n",
       "        0.114, 0.027, 0.01 , 0.021],\n",
       "       [0.015, 0.054, 0.114, 0.272, 0.035, 0.044, 0.058, 1.   , 0.169,\n",
       "        0.117, 0.096, 0.188, 0.104, 0.146, 0.15 , 0.069, 0.026, 0.01 ,\n",
       "        0.22 , 0.047, 0.015, 0.09 ],\n",
       "       [0.024, 0.061, 0.135, 0.102, 0.029, 0.046, 0.04 , 0.169, 1.   ,\n",
       "        0.084, 0.063, 0.12 , 0.106, 0.089, 0.099, 0.074, 0.058, 0.012,\n",
       "        0.123, 0.054, 0.028, 0.085],\n",
       "       [0.007, 0.022, 0.028, 0.082, 0.016, 0.024, 0.056, 0.117, 0.084,\n",
       "        1.   , 0.261, 0.039, 0.218, 0.272, 0.048, 0.145, 0.028, 0.04 ,\n",
       "        0.092, 0.016, 0.002, 0.025],\n",
       "       [0.008, 0.036, 0.05 , 0.12 , 0.02 , 0.024, 0.116, 0.096, 0.063,\n",
       "        0.261, 1.   , 0.055, 0.225, 0.35 , 0.044, 0.25 , 0.023, 0.041,\n",
       "        0.101, 0.028, 0.004, 0.021],\n",
       "       [0.015, 0.067, 0.161, 0.203, 0.01 , 0.028, 0.106, 0.188, 0.12 ,\n",
       "        0.039, 0.055, 1.   , 0.034, 0.036, 0.056, 0.083, 0.025, 0.009,\n",
       "        0.073, 0.034, 0.099, 0.061],\n",
       "       [0.009, 0.027, 0.047, 0.075, 0.028, 0.024, 0.048, 0.104, 0.106,\n",
       "        0.218, 0.225, 0.034, 1.   , 0.331, 0.058, 0.139, 0.02 , 0.032,\n",
       "        0.088, 0.042, 0.002, 0.025],\n",
       "       [0.007, 0.056, 0.052, 0.149, 0.02 , 0.029, 0.118, 0.146, 0.089,\n",
       "        0.272, 0.35 , 0.036, 0.331, 1.   , 0.09 , 0.14 , 0.053, 0.033,\n",
       "        0.161, 0.048, 0.002, 0.027],\n",
       "       [0.033, 0.147, 0.081, 0.109, 0.047, 0.05 , 0.058, 0.15 , 0.099,\n",
       "        0.048, 0.044, 0.056, 0.058, 0.09 , 1.   , 0.027, 0.11 , 0.009,\n",
       "        0.229, 0.09 , 0.008, 0.042],\n",
       "       [0.005, 0.023, 0.05 , 0.061, 0.009, 0.011, 0.082, 0.069, 0.074,\n",
       "        0.145, 0.25 , 0.083, 0.139, 0.14 , 0.027, 1.   , 0.01 , 0.061,\n",
       "        0.038, 0.018, 0.03 , 0.021],\n",
       "       [0.02 , 0.17 , 0.048, 0.069, 0.013, 0.028, 0.067, 0.026, 0.058,\n",
       "        0.028, 0.023, 0.025, 0.02 , 0.053, 0.11 , 0.01 , 1.   , 0.001,\n",
       "        0.135, 0.1  , 0.005, 0.028],\n",
       "       [0.001, 0.003, 0.009, 0.009, 0.002, 0.001, 0.006, 0.01 , 0.012,\n",
       "        0.04 , 0.041, 0.009, 0.032, 0.033, 0.009, 0.061, 0.001, 1.   ,\n",
       "        0.011, 0.002, 0.014, 0.007],\n",
       "       [0.01 , 0.148, 0.135, 0.211, 0.037, 0.048, 0.114, 0.22 , 0.123,\n",
       "        0.092, 0.101, 0.073, 0.088, 0.161, 0.229, 0.038, 0.135, 0.011,\n",
       "        1.   , 0.082, 0.006, 0.094],\n",
       "       [0.028, 0.114, 0.07 , 0.041, 0.054, 0.067, 0.027, 0.047, 0.054,\n",
       "        0.016, 0.028, 0.034, 0.042, 0.048, 0.09 , 0.018, 0.1  , 0.002,\n",
       "        0.082, 1.   , 0.005, 0.016],\n",
       "       [0.005, 0.013, 0.048, 0.065, 0.002, 0.003, 0.01 , 0.015, 0.028,\n",
       "        0.002, 0.004, 0.099, 0.002, 0.002, 0.008, 0.03 , 0.005, 0.014,\n",
       "        0.006, 0.005, 1.   , 0.023],\n",
       "       [0.011, 0.038, 0.091, 0.051, 0.009, 0.019, 0.021, 0.09 , 0.085,\n",
       "        0.025, 0.021, 0.061, 0.025, 0.027, 0.042, 0.021, 0.028, 0.007,\n",
       "        0.094, 0.016, 0.023, 1.   ]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute similarity scores\n",
    "writer_similarity_scores = cosine_similarity(final_matrix, final_matrix)\n",
    "\n",
    "writer_similarity_scores.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top similar writers and their similarity scores: [('4beacba7dc8a', 0.09675307019166673), ('15a29a4fc6ad', 0.03282717683032892), ('8a910484fe84', 0.031212698617862553)]\n"
     ]
    }
   ],
   "source": [
    "def find_top_similar_writers_with_scores(writer_id, writers_df, similarity_scores, top_n=3):\n",
    "    # Find the index of this writer in the DataFrame\n",
    "    writer_index = writers_df.index[writers_df['id'] == writer_id].tolist()[0]\n",
    "\n",
    "    # Get the similarity scores for this writer against all others\n",
    "    writer_similarity_scores = similarity_scores[writer_index]\n",
    "\n",
    "    # Sort the writers based on similarity scores in descending order, get top n+1 scores (including the writer itself)\n",
    "    sorted_indices_and_scores = sorted(enumerate(writer_similarity_scores), key=lambda x: x[1], reverse=True)[:top_n+1]\n",
    "\n",
    "    # Filter out the writer itself and limit to top n\n",
    "    top_similar = [(writers_df.iloc[i]['id'], score) for i, score in sorted_indices_and_scores if i != writer_index][:top_n]\n",
    "\n",
    "    return top_similar\n",
    "\n",
    "# Example usage\n",
    "similar_writers = find_top_similar_writers_with_scores('fca9db1c7da0', writers, writer_similarity_scores)\n",
    "print(f\"Top similar writers and their similarity scores: {similar_writers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fca9db1c7da0', '4beacba7dc8a', 0.09675307019166673),\n",
       " ('fca9db1c7da0', '15a29a4fc6ad', 0.03282717683032892),\n",
       " ('fca9db1c7da0', '8a910484fe84', 0.031212698617862553),\n",
       " ('e10ad955760c', 'c24a3d106811', 0.1696063630601724),\n",
       " ('e10ad955760c', 'b1a64eb107f0', 0.14755092545717924),\n",
       " ('e10ad955760c', '15a29a4fc6ad', 0.14693765859730573)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate over all writers to get similar writers along with similarity scores\n",
    "similar_writers_data = []\n",
    "for writer_id in writers['id']:\n",
    "    similar_writers = find_top_similar_writers_with_scores(writer_id, writers, writer_similarity_scores)\n",
    "    for sim_writer_id, score in similar_writers:\n",
    "        similar_writers_data.append((writer_id, sim_writer_id, float(score)))\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"writer_id\", StringType(), True),\n",
    "    StructField(\"similar_writer_id\", StringType(), True),\n",
    "    StructField(\"similarity_score\", FloatType(), True)\n",
    "])\n",
    "\n",
    "similar_writers_data[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list to a Spark DataFrame\n",
    "similar_writers_df = spark.createDataFrame(similar_writers_data, schema=schema)\n",
    "\n",
    "# Now you can use SparkSQL to query this DataFrame\n",
    "similar_writers_df.createOrReplaceTempView(\"similar_writers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+\n",
      "|similar_writer_id|similarity_score|\n",
      "+-----------------+----------------+\n",
      "|     d80580992695|      0.14995342|\n",
      "|     37a2cbe8bd15|      0.11766463|\n",
      "|     9b351e8113e9|      0.11608657|\n",
      "+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "writer_id = '630ab5ffdf27'  # Replace with the article ID you're interested in\n",
    "query = f\"\"\"\n",
    "SELECT similar_writer_id, similarity_score\n",
    "FROM similar_writers\n",
    "WHERE writer_id = '{writer_id}'\n",
    "ORDER BY similarity_score DESC\n",
    "LIMIT 3\n",
    "\"\"\"\n",
    "top_similar_writers = spark.sql(query)\n",
    "top_similar_writers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## recommend writers to followers based on other follower behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marklam/opt/anaconda3/envs/newproject/lib/python3.11/site-packages/sklearn/utils/extmath.py:193: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    }
   ],
   "source": [
    "# Convert the nested dictionary into a list of tuples (follower, writer)\n",
    "data_list = followers.apply(lambda row: (row['follower_id'], row['writer_ids']), axis=1).tolist()\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "df = pd.DataFrame(data_list, columns=['Follower', 'Writer'])\n",
    "\n",
    "df = df.explode('Writer').reset_index(drop=True)\n",
    "\n",
    "# Create a binary matrix\n",
    "binary_matrix = df.pivot_table(index='Follower', columns='Writer', aggfunc='size', fill_value=0)\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(binary_matrix)\n",
    "\n",
    "# Convert the similarity matrix into a DataFrame for easier readability\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=binary_matrix.index, columns=binary_matrix.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended writers for 00008456a9f0: ['15a29a4fc6ad', '37a2cbe8bd15', '4beacba7dc8a']\n"
     ]
    }
   ],
   "source": [
    "def recommend_writers_for_follower(follower_id, binary_matrix, similarity_df, top_n=3):\n",
    "    # Find the top N most similar followers\n",
    "    most_similar_followers = similarity_df[follower_id].sort_values(ascending=False).index[1:top_n+1]\n",
    "    \n",
    "    # Aggregate the writers followed by these similar followers\n",
    "    recommended_writers = binary_matrix.loc[most_similar_followers].sum().sort_values(ascending=False)\n",
    "    \n",
    "    # Exclude writers the target follower already follows\n",
    "    already_followed = binary_matrix.loc[follower_id]\n",
    "    recommended_writers = recommended_writers[already_followed == 0]\n",
    "    \n",
    "    return recommended_writers.head(top_n).index.tolist()\n",
    "\n",
    "# Example usage:\n",
    "follower_id = '00008456a9f0'  # Replace with the actual follower ID\n",
    "recommendations = recommend_writers_for_follower(follower_id, binary_matrix, similarity_df)\n",
    "print(f\"Recommended writers for {follower_id}: {recommendations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for the DataFrame\n",
    "recommendations_data = []\n",
    "for follower_id in binary_matrix.index:  # Assuming index contains follower IDs\n",
    "    recommended_writers = recommend_writers_for_follower(follower_id, binary_matrix, similarity_df)\n",
    "    for writer_id in recommended_writers:\n",
    "        recommendations_data.append((follower_id, writer_id))\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"follower_id\", StringType(), True),\n",
    "    StructField(\"recommended_writer_id\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list to a Spark DataFrame\n",
    "recommendations_df = spark.createDataFrame(recommendations_data, schema=schema)\n",
    "\n",
    "# Now you can use SparkSQL to query this DataFrame\n",
    "recommendations_df.createOrReplaceTempView(\"writer_recommendations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|recommended_writer_id|\n",
      "+---------------------+\n",
      "|         15a29a4fc6ad|\n",
      "|         37a2cbe8bd15|\n",
      "|         4beacba7dc8a|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example SparkSQL query to display recommended writers for a given follower_id\n",
    "query_follower_id = '00008456a9f0'  # Replace with the follower ID you're interested in\n",
    "query = f\"\"\"\n",
    "SELECT recommended_writer_id\n",
    "FROM writer_recommendations\n",
    "WHERE follower_id = '{query_follower_id}'\n",
    "\"\"\"\n",
    "follower_recommendations = spark.sql(query)\n",
    "follower_recommendations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
